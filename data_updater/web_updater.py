import re
import json
import time
import os
import tempfile
import requests
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from google import genai
from concurrent.futures import ThreadPoolExecutor, as_completed

# config_dataì—ì„œ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
import config_data

client = genai.Client(api_key=config_data.GOOGLE_API_KEY)


# =========================================
# 1. ê³µí†µ ìœ í‹¸
# =========================================
def get_soup(url: str):
    """URL â†’ BeautifulSoup"""
    try:
        headers = {
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36"
            )
        }
        res = requests.get(url, headers=headers, timeout=10)
        res.raise_for_status()
        if res.encoding and res.encoding.lower() == "iso-8859-1":
            res.encoding = res.apparent_encoding
        else:
            res.encoding = res.apparent_encoding
        return BeautifulSoup(res.text, "html.parser")
    except Exception as e:
        print(f"    [âŒ] ì ‘ì† ì‹¤íŒ¨ ({url}): {e}")
        return None


# =========================================
# 2. í‘œ(JSON) êµ¬ì¡° ë³€í™˜
# =========================================
def table_to_structured_data(table_soup):
    """HTML í‘œë¥¼ JSON êµ¬ì¡°(í—¤ë” + í–‰ ë¦¬ìŠ¤íŠ¸)ë¡œ ë³€í™˜"""
    headers = [th.get_text(strip=True) for th in table_soup.find_all("th")]

    if not headers:
        first_tr = table_soup.find("tr")
        if first_tr:
            headers = [td.get_text(strip=True) for td in first_tr.find_all("td")]

    rows = []
    trs = table_soup.find_all("tr")

    start_idx = 0
    if table_soup.find("thead") or (trs and trs[0].find("th")):
        start_idx = 1

    for tr in trs[start_idx:]:
        tds = tr.find_all(["td", "th"])
        if not tds:
            continue

        row = {}
        for i, td in enumerate(tds):
            if i < len(headers):
                key = headers[i]
            else:
                key = f"col_{i}"

            cell_text = td.get_text(strip=True).replace("\n", " ")
            row[key] = cell_text

        if row:
            rows.append(row)

    return {"headers": headers, "rows": rows}


# =========================================
# 3. FAQ í¬ë§·íŒ…
# =========================================
def clean_faq_content(text: str) -> str:
    """Q/A íŒ¨í„´ì„ ë§ˆí¬ë‹¤ìš´ í˜•ì‹(**Q.** / **A.**)ìœ¼ë¡œ ì •ë¦¬"""
    lines = []
    current_q = None

    for raw in text.splitlines():
        line = raw.strip()
        if not line:
            continue

        if re.match(r"^(Q[\\.:\\s]|Q$)", line):
            if current_q:
                lines.append(current_q)
            question = re.sub(r"^Q[\\s\\.:]*", "", line).strip()
            current_q = f"**Q. {question}**" if question else "**Q.**"
            continue

        if re.match(r"^(A[\\.:\\s]|A$)", line):
            answer = re.sub(r"^A[\\s\\.:]*", "", line).strip()
            if current_q:
                lines.append(current_q)
                current_q = None
            lines.append(f"**A. {answer}**" if answer else "**A.**")
            continue

        if current_q:
            lines.append(current_q)
            lines.append(f"**A. {line}**")
            current_q = None
        else:
            lines.append(line)

    if current_q:
        lines.append(current_q)

    return "\n".join(lines)


# =========================================
# 4. ì²­í‚¹ ìœ í‹¸ (RAG ìµœì í™”)
# =========================================
def split_into_paragraphs(text: str) -> list[str]:
    return [p.strip() for p in text.split("\n\n") if p.strip()]


def chunk_paragraphs(
    paragraphs,
    title: str = "",
    chunk_size: int = 500,
    overlap: int = 200,
):
    chunks: list[str] = []
    current: list[str] = []

    if title:
        current.extend([f"[TITLE] {title}"])

    for para in paragraphs:
        words = para.split()

        if len(words) > chunk_size:
            if current:
                chunks.append(" ".join(current))
                current = current[-overlap:] if len(current) > overlap else current

            for i in range(0, len(words), chunk_size - overlap):
                sub = words[i : i + chunk_size]
                chunks.append(" ".join(sub))

            continue

        if len(current) + len(words) > chunk_size:
            chunks.append(" ".join(current))
            if overlap > 0:
                current = current[-overlap:] if len(current) > overlap else current
            else:
                current = []

        current.extend(words)

    if current:
        chunks.append(" ".join(current))

    return chunks


def final_chunking(title: str, content: str) -> list[str]:
    paras = split_into_paragraphs(content)
    final_chunks = chunk_paragraphs(paras, title=title, chunk_size=500, overlap=200)
    return final_chunks


# =========================================
# 5. ìƒì„¸ í˜ì´ì§€ íŒŒì‹± (í‘œ â†’ ë³¸ë¬¸ ì‚½ì… ë¡œì§ ì ìš©)
# =========================================
def extract_content(url: str, config_item: dict | None = None):
    soup = get_soup(url)
    if not soup:
        return None

    # ê¸€ë¡œë²Œ ì¡ë™ì‚¬ë‹ˆ ì œê±°
    for tag in soup(["script", "style", "nav", "footer", "header", "iframe", "noscript", "form", "link", "meta"]):
        tag.decompose()

    target_element = soup

    # íŠ¹ì • ì˜ì—­ ì„ íƒ + ì œê±° selector ì ìš©
    if config_item:
        selector = config_item.get("content_selector")
        if selector:
            found = soup.select_one(selector)
            if found:
                target_element = found

        removes = list(config_item.get("remove_selectors", []))
        removes.extend([".list_btn", ".btn_area", ".sns_share", ".prev_next", ".file_area", ".view_nav"])
        for rm_sel in removes:
            for tag in target_element.select(rm_sel):
                tag.decompose()

    # í‘œ(Table)ë¥¼ JSON ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ë³¸ë¬¸ì— ì‹¬ê¸°
    tables_to_inject: list[tuple[str, str]] = []

    for idx, table in enumerate(target_element.find_all("table")):
        placeholder = f"___TABLE_JSON_INJECT_{idx}___"
        try:
            tbl_dict = table_to_structured_data(table)

            if not tbl_dict['headers'] and not tbl_dict['rows']:
                table.decompose()
                continue

            json_str = json.dumps(tbl_dict, ensure_ascii=False, indent=2)
            formatted_table_str = f"\n\n[TABLE]\n{json_str}\n\n"

            tables_to_inject.append((placeholder, formatted_table_str))
            table.replace_with(soup.new_string(placeholder))

        except Exception:
            pass

    # ì œëª© ì¶”ì¶œ
    title = "No Title"
    if soup.select_one(".subject"):
        title = soup.select_one(".subject").get_text().strip()
    elif soup.select_one("h3"):
        title = soup.select_one("h3").get_text().strip()
    elif soup.title:
        title = soup.title.get_text().strip()

    # í…ìŠ¤íŠ¸ ì¶”ì¶œ (Placeholder í¬í•¨ëœ ìƒíƒœ)
    text = target_element.get_text(separator="\n")

    # Placeholderë¥¼ ì‹¤ì œ JSON ë¬¸ìì—´ë¡œ ì¹˜í™˜
    for placeholder, json_str in tables_to_inject:
        text = text.replace(placeholder, json_str)

    # ë¼ì¸ ì •ì œ
    lines: list[str] = []
    skip_keywords = ["ë³¸ë¬¸ìœ¼ë¡œ ë°”ë¡œê°€ê¸°", "TOP", "List", "ê¸€ìí¬ê¸°", "SNSê³µìœ ", "ì¸ì‡„", "ë‹«ê¸°", "ëª©ë¡"]

    for line in text.splitlines():
        line = line.strip()
        if not line or line in skip_keywords:
            continue
        if line.startswith(("ì‘ì„±ì", "ì‘ì„±ì¼", "ì¡°íšŒìˆ˜")):
            continue
        lines.append(line)

    clean_body = "\n".join(lines)

    # FAQ íŒ¨í„´ ì²˜ë¦¬
    if ("Q" in clean_body and "A" in clean_body) or "faq" in url.lower():
        clean_body = clean_faq_content(clean_body)

    if len(clean_body) < 20:
        return None

    # ìµœì¢… ì²­í‚¹
    chunks = final_chunking(title, clean_body)

    return {
        "title": title,
        "url": url,
        "chunks": chunks,
        "crawled_at": time.strftime("%Y-%m-%d"),
        "chunk_count": len(chunks),
    }


# =========================================
# 6. ëª©ë¡ í˜ì´ì§€ í¬ë¡¤ë§
# =========================================
def crawl_list_page(list_url: str, link_pattern: str):
    soup = get_soup(list_url)
    if not soup:
        return []

    found_links = set()
    for a_tag in soup.find_all("a", href=True):
        href = a_tag["href"]
        if link_pattern in href:
            full_url = urljoin(list_url, href)
            found_links.add(full_url)

    return list(found_links)


# =========================================
# 7. ë©”ëª¨ë¦¬ì—ì„œ ì½˜í…ì¸  ìƒì„± (íŒŒì¼ ì €ì¥ ì•ˆ í•¨)
# =========================================
def create_web_content_chunks(records: list[dict], basename: str, batch_size: int = 40) -> list[tuple[str, str]]:
    """
    Returns: [(filename, content), ...]
    """
    chunks = []

    for i in range(0, len(records), batch_size):
        subset = records[i : i + batch_size]
        md_lines: list[str] = []

        for item in subset:
            for chunk in item["chunks"]:
                md_lines.append("### Record")
                md_lines.append(f"**Title:** {item['title']}")
                md_lines.append(f"**Link:** {item['url']}")
                md_lines.append(f"**Date:** {item['crawled_at']} (ìˆ˜ì§‘ì¼)")
                md_lines.append(f"**Chunk:**\n{chunk}")
                md_lines.append("\n---\n")

        part = i // batch_size + 1
        filename = f"{basename}_part{part}.md"
        content = "\n".join(md_lines)
        chunks.append((filename, content))

    return chunks


# =========================================
# 8. ì—…ë¡œë“œ ë° ìŠ¤í† ì–´ ë™ê¸°í™”
# =========================================
def upload_single_chunk(filename: str, content: str, store_name: str) -> tuple[str, bool, str]:
    temp_file = None
    try:
        with tempfile.NamedTemporaryFile(mode='w', encoding='utf-8', suffix='.md', delete=False) as f:
            f.write(content)
            temp_file = f.name

        op = client.file_search_stores.upload_to_file_search_store(
            file=temp_file,
            file_search_store_name=store_name,
            config={"display_name": filename, "mime_type": "text/markdown"},
        )

        while not op.done:
            time.sleep(1)
            op = client.operations.get(op)

        return (filename, True, "")

    except Exception as e:
        return (filename, False, str(e))

    finally:
        if temp_file and os.path.exists(temp_file):
            try:
                os.unlink(temp_file)
            except:
                pass


def update_store_files(store_name: str, chunks: list[tuple[str, str]], base_name_pattern: str):
    print(f"   [Store Update] '{base_name_pattern}' ë™ê¸°í™” ì‹œì‘")

    pager = client.file_search_stores.documents.list(parent=store_name)
    docs_to_delete: list[str] = []

    try:
        all_docs = list(pager)
    except Exception:
        all_docs = []

    for doc in all_docs:
        d_name = getattr(doc, "display_name", "")
        if d_name.startswith(base_name_pattern + "_part"):
            docs_to_delete.append(doc.name)

    if docs_to_delete:
        print(f"   â†’ ê¸°ì¡´ íŒŒì¼ {len(docs_to_delete)}ê°œ ì‚­ì œ ì¤‘...")
        with ThreadPoolExecutor(max_workers=5) as executor:
            for d_id in docs_to_delete:
                executor.submit(
                    client.file_search_stores.documents.delete,
                    name=d_id,
                    config={"force": True},
                )
        time.sleep(2)

    print("   â†’ ìƒˆ íŒŒì¼ ì—…ë¡œë“œ ì¤‘...")
    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = [executor.submit(upload_single_chunk, fn, ct, store_name) for fn, ct in chunks]
        for fut in as_completed(futures):
            name, ok, msg = fut.result()
            if ok:
                print(f"   âœ… {name}")
            else:
                print(f"   âŒ {name} - {msg}")

    print("   [âœ”] ë™ê¸°í™” ì™„ë£Œ\n")


# =========================================
# 9. ë©”ì¸ ì‹¤í–‰ (Auto Mode ì§€ì›)
# =========================================
def run_web_pipeline(auto_mode=None):
    store_name = config_data.AUTO_UPDATE_STORE_NAME
    web_urls = config_data.WEB_URLS

    if not web_urls:
        print("[â„¹] í¬ë¡¤ë§í•  URLì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"[âœ”] Target Store: {store_name}\n")

    if auto_mode:
        print(f"ğŸ¤– ìë™ ëª¨ë“œ(ìŠ¤ì¼€ì¤„ëŸ¬)ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤: ì˜µì…˜ {auto_mode}")
        mode = auto_mode
    else:
        print("1. ğŸ“… Daily Update (ìµœì‹ ê¸€ ìœ„ì£¼)")
        print("2. ğŸ“š Full Archive (ì „ì²´ ìˆ˜ì§‘)")
        mode = input("ì‹¤í–‰ ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš” (1/2): ").strip()

    is_daily = (mode == "1")

    for item in web_urls:
        original_name = item.get("name", "noname")
        base_url = item.get("url")
        crawl_type = item.get("type", "single")
        link_pattern = item.get("link_pattern", "")
        pagination = item.get("pagination")

        if not base_url:
            continue

        # ëª¨ë“œì— ë”°ë¥¸ í˜ì´ì§€ ë²”ìœ„ ë° íŒŒì¼ëª… ì„¤ì •
        if pagination:
            daily_limit = pagination.get("daily_limit", 5)
            full_end = pagination.get("end_page", 1)

            if is_daily:
                target_name = f"{original_name}_recent"
                p_start = 1
                p_end = daily_limit
            else:
                target_name = f"{original_name}_archive"
                p_start = daily_limit + 1
                p_end = full_end
        else:
            target_name = original_name
            p_start = 1
            p_end = 1

        print(f"=== Web Crawling: {target_name} (Page {p_start}~{p_end}) ===")
        crawled_data_list: list[dict] = []

        # [TYPE 1] ëª©ë¡í˜• ê²Œì‹œíŒ í¬ë¡¤ë§
        if crawl_type == "list" and link_pattern:
            if pagination:
                target_links = set()
                param = pagination.get("param", "nPage")

                for page_num in range(p_start, p_end + 1):
                    page_url = f"{base_url}&{param}={page_num}"
                    print(f"\r    Reading List... [Page {page_num}/{p_end}]", end="", flush=True)

                    links = crawl_list_page(page_url, link_pattern)
                    target_links.update(links)
                    time.sleep(0.3)

                print(f"\n    --> {len(target_links)}ê°œ ìƒì„¸ ë§í¬ í™•ë³´")
            else:
                target_links = set(crawl_list_page(base_url, link_pattern))

            detail_links = list(target_links)
            if detail_links:
                print(f"    2ë‹¨ê³„: ë³¸ë¬¸ í¬ë¡¤ë§ ({len(detail_links)}ê°œ)...")

                with ThreadPoolExecutor(max_workers=10) as executor:
                    future_to_url = {
                        executor.submit(extract_content, link, item): link
                        for link in detail_links
                    }

                    count = 0
                    for future in as_completed(future_to_url):
                        result = future.result()
                        if result:
                            crawled_data_list.append(result)
                            count += 1
                            if count % 10 == 0:
                                print(f"\r    - ì§„í–‰: {count}/{len(detail_links)}", end="", flush=True)
                print()

        # [TYPE 2] ë‹¨ì¼ í˜ì´ì§€ í¬ë¡¤ë§
        else:
            print("    ë‹¨ì¼ í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...")
            result = extract_content(base_url, item)
            if result:
                crawled_data_list.append(result)

        # ë©”ëª¨ë¦¬ì—ì„œ ì²­í¬ ìƒì„± ë° ì—…ë¡œë“œ
        if crawled_data_list:
            print(f"    â†’ {len(crawled_data_list)}ê°œ ë°ì´í„° ì €ì¥ ë° ë™ê¸°í™”")
            try:
                chunks = create_web_content_chunks(crawled_data_list, basename=target_name)
                if chunks:
                    update_store_files(store_name, chunks, base_name_pattern=target_name)
            except Exception as e:
                print(f"    [âŒ] ì—ëŸ¬ ë°œìƒ: {e}")
        else:
            print("    â†’ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\n")

    print("ğŸ‰ Web Pipeline ì™„ë£Œ")


if __name__ == "__main__":
    run_web_pipeline()

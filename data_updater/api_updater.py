import re
import time
import requests
import xmltodict
import os
import tempfile
from bs4 import BeautifulSoup
from google import genai
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

# API í‚¤ëŠ” config_dataì—ì„œ ê°€ì ¸ì˜´
import config_data

client = genai.Client(api_key=config_data.GOOGLE_API_KEY)


# =========================================
# 1. ê³µìš© ìœ í‹¸ (í…ìŠ¤íŠ¸ ì •ì œ/HTML ì²˜ë¦¬)
# =========================================
def clean_text(t: str) -> str:
    if not isinstance(t, str):
        t = str(t)
    t = re.sub(r"\s+", " ", t)
    return t.strip()


def looks_like_html(s: str) -> bool:
    if not s or not isinstance(s, str):
        return False
    s_lower = s.lower()
    return any(tag in s_lower for tag in ["<p", "<br", "<div", "<span", "<table", "&lt;", "&gt;", "&amp;"])


def html_to_text(raw: str) -> str:
    if not raw:
        return ""
    soup = BeautifulSoup(raw, "html.parser")
    for tag in soup(["script", "style"]):
        tag.decompose()
    return clean_text(soup.get_text(separator="\n"))


# =========================================
# 2. API í˜¸ì¶œ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
# =========================================
def fetch_api(url: str, key: str | None, retries=3):
    params = {}
    if key and "serviceKey=" not in url:
        params["serviceKey"] = key

    for attempt in range(retries):
        try:
            res = requests.get(url, params=params, timeout=20)

            if res.status_code >= 500 or res.status_code == 429:
                print(f"     [âš ï¸] ì„œë²„ ì§€ì—°({res.status_code})... ì¬ì‹œë„ {attempt+1}/{retries}")
                time.sleep(2)
                continue

            if 400 <= res.status_code < 500:
                print(f"     [âŒ] ìš”ì²­ ì˜¤ë¥˜: {res.status_code} (í‚¤/URL í™•ì¸)")
                return None

            res.raise_for_status()

            ct = res.headers.get("Content-Type", "").lower()
            if "json" in ct:
                return res.json()
            if "xml" in ct or res.text.strip().startswith("<"):
                try:
                    return xmltodict.parse(res.text)
                except:
                    pass

            soup = BeautifulSoup(res.text, "html.parser")
            return soup.get_text(separator="\n", strip=True)

        except requests.exceptions.RequestException as e:
            print(f"     [âš ï¸] ì—°ê²° ì‹¤íŒ¨: {e}... ì¬ì‹œë„ {attempt+1}/{retries}")
            time.sleep(2)

    print(f"     [âŒ] {retries}íšŒ ì‹¤íŒ¨.")
    return None


# =========================================
# 3. ë°ì´í„° ì¶”ì¶œ ê´€ë ¨ í•¨ìˆ˜ë“¤
# =========================================
def extract_items(data):
    if isinstance(data, list):
        if all(isinstance(x, dict) for x in data): return data
        return data

    if isinstance(data, dict):
        candidates = []
        for v in data.values():
            r = extract_items(v)
            if isinstance(r, list): candidates.append(r)
        if candidates: return max(candidates, key=len)
    return None


def flatten_dict(obj, prefix="", out=None):
    if out is None: out = {}
    if isinstance(obj, dict):
        for k, v in obj.items():
            flatten_dict(v, prefix + k + "_", out)
    elif isinstance(obj, list):
        for i, v in enumerate(obj):
            flatten_dict(v, prefix + str(i) + "_", out)
    else:
        if obj not in [None, ""]:
            val = str(obj)
            if looks_like_html(val): val = html_to_text(val)
            out[prefix[:-1]] = clean_text(val)
    return out


def pick_title(flat: dict):
    for k, v in flat.items():
        if any(t in k.lower() for t in ["title", "name", "headline", "program"]): return v
    return None


def pick_date(flat: dict):
    for k, v in flat.items():
        if any(t in k.lower() for t in ["regdate", "date", "created", "updated", "start"]): return v
    return None


def pick_link(flat: dict):
    for k, v in flat.items():
        if any(t in k.lower() for t in ["url", "link", "href"]): return v
    return None


def pick_description(flat: dict):
    key_based = []
    for k, v in flat.items():
        if any(t in k.lower() for t in ["description", "desc", "content", "body", "summary", "info"]):
            key_based.append(v)
    if key_based:
        key_based.sort(key=len, reverse=True)
        return key_based[0]

    value_candidates = []
    for k, v in flat.items():
        if any(x in k.lower() for x in ["title", "name", "date", "id", "code", "url", "link"]): continue
        if len(v) >= 30: value_candidates.append(v)
    if value_candidates:
        value_candidates.sort(key=len, reverse=True)
        return value_candidates[0]
    return None


def parse_date_str(s: str):
    if not s: return datetime.min
    s = str(s).replace("/", "-").strip()
    if " " in s: s = s.split(" ")[0]
    for fmt in ["%Y-%m-%d", "%Y.%m.%d", "%Y%m%d"]:
        try: return datetime.strptime(s, fmt)
        except: continue
    return datetime.min


def sort_items_by_date(items):
    def key_fn(rec):
        flat = flatten_dict(rec)
        d = pick_date(flat)
        return parse_date_str(d or "")
    return sorted(items, key=key_fn, reverse=True)


def format_record(rec) -> str:
    flat = flatten_dict(rec)
    title = pick_title(flat)
    date = pick_date(flat)
    link = pick_link(flat)
    desc = pick_description(flat)

    lines = ["### Record"]
    if title: lines.append(f"**Title:** {title}")
    if date: lines.append(f"**Date:** {date}")
    if desc: lines.append(f"**Description:** {desc}")
    else: lines.append(f"**Description:** (ë‚´ìš© ì—†ìŒ)")
    if link: lines.append(f"**Link:** {link}")

    lines.append("\n**Details:**")
    for k, v in flat.items():
        if v in [title, date, desc, link] or not v: continue
        lines.append(f"- {k}: {v}")
    return "\n".join(lines) + "\n\n---\n\n"


# =========================================
# 4. ë©”ëª¨ë¦¬ì—ì„œ ì²­í‚¹ (íŒŒì¼ ì €ì¥ ì—†ìŒ)
# =========================================
def create_chunks_in_memory(items, basename: str, batch_size: int = 100) -> list[tuple[str, str]]:
    """
    Returns: [(filename, content), ...]
    """
    chunks = []
    total_items = len(items)
    if total_items == 0:
        return []

    for i in range(0, total_items, batch_size):
        chunk_items = items[i : i + batch_size]
        chunk_text = "".join(format_record(it) for it in chunk_items)
        part_num = (i // batch_size) + 1

        filename = f"{basename}_part{part_num}.md"
        chunks.append((filename, chunk_text))

    return chunks


# =========================================
# 5. ì„ì‹œ íŒŒì¼ ì—…ë¡œë“œ (ë©”ëª¨ë¦¬ -> ì„ì‹œ íŒŒì¼ -> ì—…ë¡œë“œ -> ì‚­ì œ)
# =========================================
def upload_single_chunk(filename: str, content: str, store_name: str, max_wait: int = 120) -> tuple[str, bool, str]:
    """
    ë©”ëª¨ë¦¬ì—ì„œ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥ í›„ ì—…ë¡œë“œ, ì—…ë¡œë“œ í›„ ì„ì‹œ íŒŒì¼ ì‚­ì œ
    """
    temp_file = None
    try:
        # ì„ì‹œ íŒŒì¼ ìƒì„±
        with tempfile.NamedTemporaryFile(mode='w', encoding='utf-8', suffix='.md', delete=False) as f:
            f.write(content)
            temp_file = f.name

        # ì—…ë¡œë“œ
        op = client.file_search_stores.upload_to_file_search_store(
            file=temp_file,
            file_search_store_name=store_name,
            config={
                "display_name": filename,
                "mime_type": "text/markdown"
            }
        )

        wait_sec = 0
        while not op.done:
            time.sleep(1)
            op = client.operations.get(op)
            wait_sec += 1
            if wait_sec > max_wait:
                return (filename, False, f"íƒ€ì„ì•„ì›ƒ ({max_wait}ì´ˆ)")

        return (filename, True, "")

    except Exception as e:
        return (filename, False, str(e))

    finally:
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        if temp_file and os.path.exists(temp_file):
            try:
                os.unlink(temp_file)
            except:
                pass


def parallel_upload_chunks(chunks: list[tuple[str, str]], store_name: str, max_workers: int = 5):
    """
    chunks: [(filename, content), ...]
    """
    print(f"   â†’ ìƒˆ íŒŒì¼ {len(chunks)}ê°œ ë³‘ë ¬ ì—…ë¡œë“œ ì¤‘...")

    success_count = 0
    failed_files = []

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_chunk = {
            executor.submit(upload_single_chunk, fname, content, store_name): fname
            for fname, content in chunks
        }

        for future in as_completed(future_to_chunk):
            d_name, success, error_msg = future.result()

            if success:
                print(f"     âœ… {d_name}")
                success_count += 1
            else:
                print(f"     âŒ {d_name} - {error_msg}")
                failed_files.append((d_name, error_msg))

    print(f"   â†’ ì—…ë¡œë“œ ì™„ë£Œ: ì„±ê³µ {success_count}/{len(chunks)}")
    if failed_files:
        print(f"   âš ï¸ ì‹¤íŒ¨: {len(failed_files)}ê°œ")


# =========================================
# 6. FileSearchStore ì—…ë°ì´íŠ¸
# =========================================
def update_store_files(store_name: str, chunks: list[tuple[str, str]], base_name_pattern: str):
    print(f"   [Store Update] '{base_name_pattern}' ë™ê¸°í™” ì‹œì‘")

    # 1) ê¸°ì¡´ ë¬¸ì„œ ê²€ìƒ‰ ë° ì‚­ì œ
    pager = client.file_search_stores.documents.list(parent=store_name)

    docs_to_delete = []
    for doc in pager:
        d_name = getattr(doc, "display_name", "")
        if d_name.startswith(base_name_pattern + "_part"):
            docs_to_delete.append(doc.name)

    if docs_to_delete:
        print(f"   â†’ ê¸°ì¡´ íŒŒì¼ {len(docs_to_delete)}ê°œ ì‚­ì œ ì¤‘...")
        for d_id in docs_to_delete:
            try:
                client.file_search_stores.documents.delete(name=d_id, config={"force": True})
            except Exception:
                pass
        time.sleep(2)

    # 2) ìƒˆ íŒŒì¼ë“¤ ë³‘ë ¬ ì—…ë¡œë“œ
    parallel_upload_chunks(chunks, store_name, max_workers=5)

    print("   [âœ”] ë™ê¸°í™” ì™„ë£Œ\n")


# =========================================
# 7. íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
# =========================================
def run_api_pipeline():
    store_name = config_data.AUTO_UPDATE_STORE_NAME
    apis = config_data.APIS

    print(f"[âœ”] Target Store: {store_name}\n")

    success_apis = []
    failed_apis = []

    for api in apis:
        name = api["name"]
        url = api["url"]
        key_env = api.get("key_env")
        key = os.getenv(key_env) if key_env else None

        print(f"=== API ì²˜ë¦¬: {name} ===")

        data = fetch_api(url, key)
        if data is None:
            failed_apis.append((name, "API ì‘ë‹µ ì‹¤íŒ¨"))
            print("   â†’ ì‹¤íŒ¨ (API Error)\n")
            continue

        items = extract_items(data) or [data]
        if isinstance(items, dict): items = [items]

        items_sorted = sort_items_by_date(items)
        print(f"   â†’ {len(items_sorted)}ê°œ ì•„ì´í…œ ì¶”ì¶œë¨")

        try:
            # ë©”ëª¨ë¦¬ì—ì„œ ì²­í‚¹ (íŒŒì¼ ì €ì¥ ì•ˆ í•¨)
            chunks = create_chunks_in_memory(items_sorted, basename=name, batch_size=100)

            if chunks:
                update_store_files(store_name, chunks, base_name_pattern=name)
                success_apis.append(name)
            else:
                print("   â†’ ì €ì¥í•  ë°ì´í„° ì—†ìŒ\n")
        except Exception as e:
            failed_apis.append((name, str(e)))
            print(f"   â†’ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {e}\n")

    print("\n====================")
    print("ğŸ‰ API ì—…ë°ì´íŠ¸ ì™„ë£Œ")
    print("   ì„±ê³µ:", success_apis)
    print("   ì‹¤íŒ¨:", [f[0] for f in failed_apis])
    print("====================")


if __name__ == "__main__":
    run_api_pipeline()
